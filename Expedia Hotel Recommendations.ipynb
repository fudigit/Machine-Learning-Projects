{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False:\n",
    "    # Load destinations\n",
    "    import pandas as pd\n",
    "    destinations = pd.read_csv('destinations.csv')\n",
    "    destinations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data size: (2528243, 22)\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#load test set\n",
    "test_set = pd.read_csv('test.csv')\n",
    "print('test data size:', test_set.shape)\n",
    "#test_set.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size (37670293, 24)\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False:\n",
    "    # subset with 10k user_id -- the train_sel_10k was created in the directory earlier. No need to load the whole train\n",
    "    sel_train = pd.read_csv('sel_train_10k.csv')\n",
    "    \n",
    "    \n",
    "if True:\n",
    "    # load train set, the dataset is very large, we will work with a subset: (37670293, 24)\n",
    "    # take 5 min to load\n",
    "    train = pd.read_csv('train.csv')\n",
    "    print('train data size', train.shape)\n",
    "    #train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.info()\n",
    "#test.shape\n",
    "#test.info()\n",
    "#train.head(5)\n",
    "#test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we predicting? Given search, predict the hotel cluter booked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are we scored? Mean Average Precision @ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # distribution of hotel cluster\n",
    "    %matplotlib inline\n",
    "    sel_train['hotel_cluster'].hist(bins = 100)\n",
    "    # distribution of count for each hotel cluster:\n",
    "    #sel_train['hotel_cluster'].value_counts().hist(bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test user id is a subet of train's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest_ids = set(test['user_id'])\\ntrain_ids = set(train['user_id'])\\nintersection_count = len(test_ids & train_ids)\\nprint('test&train common unique ids:',intersection_count, 'test unique ids', len(test_ids))\\n\""
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "test_ids = set(test['user_id'])\n",
    "train_ids = set(train['user_id'])\n",
    "intersection_count = len(test_ids & train_ids)\n",
    "print('test&train common unique ids:',intersection_count, 'test unique ids', len(test_ids))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick 10000 users with random user ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # when the full train is loaded, pick the subset and write it to the directory\n",
    "    import random\n",
    "    sel_user_id = random.sample(train_ids, 10000)\n",
    "    sel_train = train[train['user_id'].isin(sel_user_id)]\n",
    "    sel_train.to_csv('sel_train_10k.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    sel_train[\"date_time\"] = pd.to_datetime(sel_train[\"date_time\"])\n",
    "    #sel_train[\"year\"] = sel_train[\"date_time\"].dt.year\n",
    "    #sel_train[\"month\"] = sel_train[\"date_time\"].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribution of counts of each user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sel_train.groupby(sel_train['user_id'])['user_id'].count().hist(range = [0, 100], bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sel_train.groupby('user_id')['site_name'].count().reset_index(name = 'count').sort_values(['count'], ascending=False)\n",
    "#sel_train['user_id'].value_counts()\n",
    "# equivalent to value_counts for series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribution of counts of each day/month, Seperate to new training and testing set based on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the time to be the index, this ease the date split\n",
    "if False:\n",
    "    \n",
    "    sel_train = sel_train.set_index('date_time')\n",
    "    sel_train['site_name'].resample('D').count().plot()\n",
    "    sel_train['site_name'].resample('M').count().plot()\n",
    "\n",
    "    ## Seperate to new training and testing set based on date\n",
    "    sel_train\n",
    "    len(sel_train['user_id'].unique())\n",
    "    t1 = sel_train[sel_train.index < '2014-08-01']\n",
    "    t2 = sel_train[sel_train.index >= '2014-08-01'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove click events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # when t2 is not the kaggle set,  t2['is_booking'] exists\n",
    "    t2_b = t2[t2['is_booking'] == 1]\n",
    "    t2_b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set variables of trainset and testset, can use local or Kaggle's Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    #t1 = sel_train\n",
    "    t1 = train\n",
    "    t2 = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple algorithm - predict as top 5 common cluster for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 526 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if False:\n",
    "    most_common_clusters = list(sel_train.hotel_cluster.value_counts().head(5).index)\n",
    "\n",
    "if True:\n",
    "    most_common_clusters = list(t1.hotel_cluster.value_counts().head(5).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # when t2 has the ground truth label\n",
    "    predictions = [most_common_clusters for _ in range(t2.shape[0])]\n",
    "    target = [[l] for l in t2['hotel_cluster']]\n",
    "\n",
    "    import ml_metrics as metrics\n",
    "    metrics.mapk(target, predictions, k=5)\n",
    "    # check how mapk is calculated\n",
    "    #[(apk(a,p,5),a,p) for a,p in zip(target, predictions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no column correlates to hotel cluster\n",
    "#t1.corr()[\"hotel_cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features (for machine learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    destinations.head()\n",
    "    from sklearn.decomposition import PCA\n",
    "    # only get the top 3 PCs\n",
    "    pca = PCA(n_components =3)\n",
    "    # transform all features to 3 PCs, call dest_small\n",
    "    dest_small = pca.fit_transform(destinations[['d{0}'.format(i+1) for i in range(0, 149)]])\n",
    "    dest_small = pd.DataFrame(dest_small)\n",
    "    dest_small['srch_destination_id'] = destinations['srch_destination_id']\n",
    "    \n",
    "    ## Feature based oï¼š train set + 3 Principal components from destinations\n",
    "    # create date column based on the date index\n",
    "    t1.loc[:, 'date_time'] = t1.index\n",
    "    \n",
    "    def calc_fast_features(t1):\n",
    "\n",
    "        # convert to date_time format\n",
    "        t1[\"date_time\"] = pd.to_datetime(t1[\"date_time\"])\n",
    "        t1[\"srch_ci\"] = pd.to_datetime(t1[\"srch_ci\"], format='%Y-%m-%d', errors=\"coerce\")\n",
    "        t1[\"srch_co\"] = pd.to_datetime(t1[\"srch_co\"], format='%Y-%m-%d', errors=\"coerce\")\n",
    "\n",
    "        # quarter, month, day of week of the search log timestamp\n",
    "        props = {}\n",
    "        for prop in [\"quarter\", \"month\", \"dayofweek\", \"day\", \"hour\", \"minute\"]:\n",
    "            props[prop] = getattr(t1['date_time'].dt, prop)\n",
    "\n",
    "        # rest of the columns other than date_time, srch_ci/srch_co\n",
    "        carryover = [p for p in t1.columns if p not in [\"date_time\", \"srch_ci\", \"srch_co\"]]\n",
    "        for prop in carryover:\n",
    "            props[prop] = t1[prop]\n",
    "\n",
    "        # quarter, month, dayofweek, and day of chech in/check out date     \n",
    "        date_props = [\"month\", \"day\", \"dayofweek\", \"quarter\"]\n",
    "        for prop in date_props:\n",
    "            props[\"ci_{}\".format(prop)] = getattr(t1[\"srch_ci\"].dt, prop)\n",
    "            props[\"co_{}\".format(prop)] = getattr(t1[\"srch_co\"].dt, prop)\n",
    "\n",
    "        # length of stay\n",
    "        props[\"stay_span\"] = (t1[\"srch_co\"] - t1[\"srch_ci\"]).astype('timedelta64[h]')\n",
    "\n",
    "        ret = pd.DataFrame(props)\n",
    "\n",
    "        # merge the destination feature\n",
    "\n",
    "        ret = ret.join(dest_small, on = \"srch_destination_id\", how = 'left', rsuffix = \"_dest\")\n",
    "        ret = ret.drop('srch_destination_id_dest', axis =1)\n",
    "        return ret\n",
    "\n",
    "    df = calc_fast_features(t1)\n",
    "    df.fillna(-1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning attempt (show how a typical ML works)\n",
    "### 1. This is predicting hotel cluster given a search event, no matter a booking or click event.\n",
    "### 2. Simple random forest only predict 1 cluster, using accuracy for score, not map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple random forest with all features created\n",
    "\n",
    "if False:\n",
    "    df_rf = df[df[\"is_booking\"] == 1]\n",
    "    predictors = [c for c in df.columns if c not in [\"hotel_cluster\"]]\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=10, max_depth = 50, min_weight_fraction_leaf=0.1)\n",
    "    # this score is using mean accuracy\n",
    "    scores = cross_val_score(clf, df_rf[predictors], df_rf['hotel_cluster'], cv=2)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm what the cross_val_score is doing, test KFold \n",
    "if False:\n",
    "    # split the data into train and test, I only get the indices, not actual split\n",
    "    from sklearn.model_selection import KFold\n",
    "    KFold(2).split(df)\n",
    "    for train, test in KFold(2).split(df):\n",
    "        print(train, test)\n",
    "    # train model and make predictions \n",
    "    clf.fit(df[predictors][98986:], df['hotel_cluster'][98986:])\n",
    "    predictions_raw = clf.predict(df[predictors][:98986])\n",
    "\n",
    "    ## test results\n",
    "    # ML is doing somework, this is better than the baseline result\n",
    "    target = [[l] for l in df[\"hotel_cluster\"][:98986]]\n",
    "    predictions = [[p] for p in predictions_raw]\n",
    "    mapk(target, predictions, k=5)\n",
    "    \n",
    "    # no matter booking or no booking\n",
    "    df['is_booking'].groupby(df['is_booking']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score of predicting a random cluster, a naive baseline\n",
    "if False:\n",
    "    # mapk of predicting a random cluster\n",
    "    unique_clusters = set(df['hotel_cluster'].unique())\n",
    "    import random\n",
    "    predictions_random = [random.sample(unique_clusters,1) for _ in range(t2.shape[0])]\n",
    "    mapk(target, predictions_random, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import KFold\n",
    "    from itertools import chain\n",
    "    \n",
    "    #Build Binary Classifier for each hotel cluster\n",
    "    #1. Loop through each hotel cluster.\n",
    "    #  1. Create the new binary \"label\" for each row, indicates whether the row is in the cluster, or not.\n",
    "    #  2. Train binary classifier for each of the K-fold and make predictions, extract the probablities of the cluster is \"booked\"\n",
    "    #  3. Append the probabilities for a cluster to the whole probability list\n",
    "    #2. Convert the whole probability list to data frame, transpose, add the column headers\n",
    "    #3. For each row, find the 5 largest probabilities, and assign the hotel_cluster value as predictions\n",
    "    #4. Compute accuracy via mapk\n",
    "    \n",
    "    all_probs = []\n",
    "    unique_clusters = df['hotel_cluster'].unique()\n",
    "    #1.\n",
    "    for cluster in unique_clusters:\n",
    "        df['target'] = 0\n",
    "        df['target'][df['hotel_cluster'] == cluster] = 1\n",
    "        predictors = [col for col in df.columns if col not in ['hotel_cluster','target']]\n",
    "        probs = []\n",
    "        cv = KFold(n_splits = 2)\n",
    "        clf = RandomForestClassifier(n_estimators=10, min_weight_fraction_leaf=0.1)\n",
    "\n",
    "        for train_index, test_index in cv.split(df):\n",
    "            clf.fit(df[predictors].iloc[train_index], df[\"target\"].iloc[train_index])\n",
    "            preds = clf.predict_proba(df[predictors].iloc[test_index])\n",
    "            probs.append([p[1] for p in preds])\n",
    "        \n",
    "        full_probs = chain.from_iterable(probs)\n",
    "        all_probs.append(list(full_probs))\n",
    "    #2.\n",
    "    prediction_frame = pd.DataFrame(all_probs).T\n",
    "    prediction_frame.columns = unique_clusters\n",
    "    #3.\n",
    "    def find_top_5(row):\n",
    "        return list(row.nlargest(5).index)\n",
    "    preds = []\n",
    "\n",
    "    for index, row in prediction_frame.iterrows():\n",
    "        preds.append(find_top_5(row))\n",
    "    #4.\n",
    "    mapk([[l] for l in df[\"hotel_cluster\"]], preds, k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top cluster for each srch_destination_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def make_key(items):\n",
    "    return \"_\".join([str(i) for i in items])\n",
    "\n",
    "match_cols = [\"srch_destination_id\"]\n",
    "cluster_cols = match_cols + ['hotel_cluster']\n",
    "# group by a list of columns\n",
    "groups = t1.groupby(cluster_cols)\n",
    "\n",
    "top_clusters = {}\n",
    "# this return the groupby indices as a tuple, and the grouped data\n",
    "for name, group in groups:\n",
    "    \n",
    "    clicks = len(group.is_booking[group.is_booking == False])\n",
    "    bookings = len(group.is_booking[group.is_booking == True])\n",
    "\n",
    "    score = bookings + .15 * clicks\n",
    "\n",
    "    clus_name = make_key(name[:len(match_cols)])\n",
    "    # if the srch_dest_id is not in the dict, set key as the srch_dest_id, value as another dict\n",
    "    if clus_name not in top_clusters:\n",
    "        top_clusters[clus_name] = {}\n",
    "    # now the srch_dest_id is in the dict, find the dict for the srch_dest_id, add key = cluster, value = score\n",
    "    top_clusters[clus_name][name[-1]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform the dictionary to find the top k clusters\n",
    "import operator\n",
    "cluster_dict = {}\n",
    "for srch_dest in top_clusters:\n",
    "    cluster_w_score = top_clusters[srch_dest]\n",
    "    #rank_top_5 = sorted(cluster_w_score, key = o)\n",
    "    top_5 = [l[0] for l in sorted(cluster_w_score.items(), key = operator.itemgetter(1), reverse = True)[:5]]\n",
    "    cluster_dict[srch_dest] = top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for t2: for each row in t2, use its srch_destination_id to find the top 5, append empty if not exist\n",
    "preds = []\n",
    "for srch_destination_id in t2['srch_destination_id']:\n",
    "    if str(srch_destination_id) in cluster_dict:\n",
    "        pred = cluster_dict[str(srch_destination_id)]\n",
    "        preds.append(pred)\n",
    "    else:\n",
    "        preds.append([])\n",
    "\n",
    "if False:\n",
    "    # if the ground truth is known\n",
    "    print(mapk([[l] for l in t2[\"hotel_cluster\"]], preds, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leakage solution: finding the exact matching users with leak of orig_destination_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why leakage solution works? The distance defines the cluter!\n",
    "#pd.set_option('display.max_columns', 999)\n",
    "#t1[t1['user_id'] == 15286]\n",
    "#t1[['user_id','is_booking', 'user_location_country', 'user_location_region', 'user_location_city', 'hotel_market', 'orig_destination_distance', 'hotel_cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_cols = ['user_location_country', 'user_location_region', 'user_location_city', 'hotel_market', 'orig_destination_distance']\n",
    "\n",
    "groups = t1.groupby(match_cols)\n",
    "\n",
    "def generate_exact_matches(row, match_cols):\n",
    "    index = tuple([row[t] for t in match_cols])\n",
    "    # see if the t2 cols can be found in t1\n",
    "    try:\n",
    "        group = groups.get_group(index)\n",
    "    except Exception:\n",
    "        return []\n",
    "    # if found, return hotel_cluster(s)\n",
    "    clus = list(set(group.hotel_cluster))\n",
    "    return clus\n",
    "\n",
    "exact_matches = []\n",
    "for i in range(t2.shape[0]):\n",
    "    exact_matches.append(generate_exact_matches(t2.iloc[i], match_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 37, 55, 11, 22, 91, 41, 48, 64, 65]"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_matches[0] + preds[0] + most_common_clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine 3 solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Di Fu\\\\Desktop\\\\Di\\\\Expedia'"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f5(seq, idfun=None):\n",
    "    if idfun is None:\n",
    "        def idfun(x): return x\n",
    "    seen = {}\n",
    "    result = []\n",
    "    for item in seq:\n",
    "        marker = idfun(item)\n",
    "        if marker in seen: continue\n",
    "        seen[marker] = 1\n",
    "        result.append(item)\n",
    "    return result\n",
    "\n",
    "full_preds = [f5(exact_matches[p] + preds[p] + most_common_clusters)[:5] for p in range(len(preds))]\n",
    "\n",
    "if False:\n",
    "    # work when t2['hotel_cluster'] exists\n",
    "    mapk([[l] for l in t2['hotel_cluster']], full_preds, k = 5)\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write result to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_p = [\" \".join([str(l) for l in p]) for p in full_preds]\n",
    "write_frame = [\"{0},{1}\".format(test_set[\"id\"][i], write_p[i]) for i in range(len(full_preds))]\n",
    "write_frame = [\"id,hotel_cluster\"] + write_frame\n",
    "with open(\"predictions.csv\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(write_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_set = pd.read_csv('predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2528243"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2528243"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(upload_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
